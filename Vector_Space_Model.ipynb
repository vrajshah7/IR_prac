{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Vector Space Model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXuVi0vqZjBu"
      },
      "source": [
        "** Vector Space Model**\n",
        "\n",
        "I have done this project on Google Colaboratory. So you can just click this shareable link (https://colab.research.google.com/drive/1Oio0HMNVipLcO_vTrNiaQxB45ef0TgqJ) and run.\n",
        "\n",
        "OR you can download the notebook and run on local machine or whatever you want.\n",
        "\n",
        "I will upadate the notebook with more optimized code. This time it takes a lot of time and memory.\n",
        "\n",
        "LINK FOR DATASET: https://drive.google.com/open?id=10pTRD0Yg-QR3v_j7Rr-RAT2HiN8Ro6Pq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVWDX7gNZt3r"
      },
      "source": [
        "**Objective:** \n",
        "\n",
        "*Objective of this project is to implement the Vector Space Model (VSM). Dataset contains a collection of about 22,000 text files.*\n",
        "\n",
        "*Assigned task was to implement a VSM and given a query, program should return top 5 documents related to the query.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olrQF1eM1Ts8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6990d8ca-bb13-44a1-9532-9409d1b7f112"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZzylvqDzBC_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b1d33fb-027d-47ad-f358-8c5da3f9eeac"
      },
      "source": [
        "from zipfile import ZipFile\n",
        "file_name = '/content/drive/My Drive/ACL Text.zip'\n",
        "\n",
        "with ZipFile(file_name, 'r',) as zip:\n",
        "  zip.extractall()\n",
        "  print('Done!!')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mv4ngG72aHKK"
      },
      "source": [
        "**Bellow cell imports all the necessary libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkFO0aH-8W5g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf80d20b-f026-4cbd-9569-5b1ec39fae48"
      },
      "source": [
        "import glob\n",
        "import nltk\n",
        "nltk.download('popular');\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "import string\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "from collections import OrderedDict"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJd9bSh7aRn5"
      },
      "source": [
        "**give_path: **function** **\n",
        "\n",
        "In below cell, give_path() takes path of the folder containing all the documents.\n",
        "give_path() then read all the documents and store content of each document (in string format) as 'value' of dictionary and it's file name as 'key' in the dictionary.\n",
        "This function the returns the dictionary containing all documents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJK8PfVs8q1u"
      },
      "source": [
        "def give_path(fld_path):                             #give path of the folder containing all documents\n",
        "    dic = {}\n",
        "    file_names = glob.glob(fld_path)\n",
        "    files_150 = file_names[0:10]\n",
        "    for file in files_150:\n",
        "        name = file.split('/')[-1]\n",
        "        with open(file, 'r', errors='ignore') as f:\n",
        "            data = f.read()\n",
        "        dic[name] = data\n",
        "    return dic"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2f2SMGJaVnO"
      },
      "source": [
        "**wordList_removePuncs: **function** **\n",
        "\n",
        "In the below cell, two functions wordList() and removePuncs() functions have been combined as wordList_removePuncs.\n",
        "This function is passed with dictionary of all documents (returned by give_path()). Function then remove the stop words and punctuations and returns a list of all the words in our collection of documents. This list contains the redundant words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxmwGJ8V9Wco"
      },
      "source": [
        "def wordList_removePuncs(doc_dict):\n",
        "    stop = stopwords.words('english') + list(string.punctuation) + ['\\n']\n",
        "    wordList = []\n",
        "    for doc in doc_dict.values():\n",
        "        for word in word_tokenize(doc.lower().strip()): \n",
        "            if not word in stop:\n",
        "                wordList.append(word)\n",
        "    return wordList"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBS9u85AaZb8"
      },
      "source": [
        "**termFrequencyInDoc: **function** **\n",
        "\n",
        "After the above function returns a list of all words in the collection of documents, vocabulary of words is made (I'm making vocabulary in 'main'). This vocabulary and collection of documents in dictionary form (returned by give_path) is passed to below function (termFrequencyInDoc) to find the frequency of words in each documet. This function returns a dictionary in which 'key' is the document name and its 'value' is another dicionary, containing a word as 'key' and its frequency as 'value'. It's actually a dictionary within a dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mt1vdt949b8M"
      },
      "source": [
        "def termFrequencyInDoc(vocab, doc_dict):\n",
        "    tf_docs = {}\n",
        "    for doc_id in doc_dict.keys():\n",
        "        tf_docs[doc_id] = {}\n",
        "    \n",
        "    for word in vocab:\n",
        "        for doc_id,doc in doc_dict.items():\n",
        "            tf_docs[doc_id][word] = doc.count(word)\n",
        "    return tf_docs"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mJs2Xr5ad4x"
      },
      "source": [
        "**wordDocFre: **function** **\n",
        "\n",
        "To count the document frequency, vocabulary of words and dictionary of collection of documents if passed to wordDocFre which returns a dictionary containing a word as 'key' and its document frequency as 'value' of dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1HuAAEi9d0t"
      },
      "source": [
        "def wordDocFre(vocab, doc_dict):\n",
        "    df = {}\n",
        "    for word in vocab:\n",
        "        frq = 0\n",
        "        for doc in doc_dict.values():\n",
        "#             if word in doc.lower().split():\n",
        "            if word in word_tokenize(doc.lower().strip()):\n",
        "                frq = frq + 1\n",
        "        df[word] = frq\n",
        "    return df"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DfO6susakIx"
      },
      "source": [
        "**inverseDocFre: **function** **\n",
        "\n",
        "inverseDocFre calculates the inverse document frequency. It takes vocabulary of words, dictionary of document frequency (returned by wordDocFre) and number of documents in our collection. It returns a dictionary containing inverse DF in which a word is key and its idf score is its value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrgQ8bks9fvW"
      },
      "source": [
        "def inverseDocFre(vocab,doc_fre,length):\n",
        "    idf= {} \n",
        "    for word in vocab:     \n",
        "        idf[word] = np.log2((length+1) / doc_fre[word])\n",
        "    return idf"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0BEtQfuao7Q"
      },
      "source": [
        "**tfidf: **function** **\n",
        "\n",
        "Function tfidf takes 4 arguments:\n",
        "1. vocabular of words \n",
        "2. term frequencies: which is passed in form of a dictionary\n",
        "3. Inverse DF: passed in form of a dictionary\n",
        "4. Collection of all docs passed in form of a dictionary\n",
        "\n",
        "It returns a dictionary, which is again a dictionary within a dictionary like in case of TF, but it contains score values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unCpXOmt9hnb"
      },
      "source": [
        "def tfidf(vocab,tf,idf_scr,doc_dict):\n",
        "    tf_idf_scr = {}\n",
        "    for doc_id in doc_dict.keys():\n",
        "        tf_idf_scr[doc_id] = {}\n",
        "    for word in vocab:\n",
        "        for doc_id,doc in doc_dict.items():\n",
        "            tf_idf_scr[doc_id][word] = tf[doc_id][word] * idf_scr[word]\n",
        "    return tf_idf_scr"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwnqluTnaxrw"
      },
      "source": [
        "**vectorSpaceModel: **function** **\n",
        "\n",
        "To find the relevant documents related to query, pass the query to function alonwith collection of documents (dictionary) and tf-idf scores (dictionary returned by tfidf). Function returns the top 5 documents from a collection of about 22k documents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69u58ZAI9kDm"
      },
      "source": [
        "def vectorSpaceModel(query, doc_dict,tfidf_scr):\n",
        "    query_vocab = []\n",
        "    for word in query.split():\n",
        "        if word not in query_vocab:\n",
        "            query_vocab.append(word)\n",
        "\n",
        "    query_wc = {}\n",
        "    for word in query_vocab:\n",
        "        query_wc[word] = query.lower().split().count(word)\n",
        "    \n",
        "    relevance_scores = {}\n",
        "    for doc_id in doc_dict.keys():\n",
        "        score = 0\n",
        "        for word in query_vocab:\n",
        "            score += query_wc[word] * tfidf_scr[doc_id][word]\n",
        "        relevance_scores[doc_id] = score\n",
        "    sorted_value = OrderedDict(sorted(relevance_scores.items(), key=lambda x: x[1], reverse = True))\n",
        "    top_5 = {k: sorted_value[k] for k in list(sorted_value)[:5]}\n",
        "    return top_5"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwI0Or1Ba2N9"
      },
      "source": [
        "**Here is the main function call which calls all the functions above and gives the results.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAYe0y7KGIdW"
      },
      "source": [
        "if __name__  == \"__main__\":\n",
        "   path = '/content/ACL txt/*.txt'\n",
        "   docs = give_path(path)                        #returns a dictionary of all docs\n",
        "   M = len(docs)                                 #number of files in dataset\n",
        "   w_List = wordList_removePuncs(docs)           #returns a list of tokenized words\n",
        "   vocab = list(set(w_List))                     #returns a list of unique words\n",
        "   tf_dict = termFrequencyInDoc(vocab, docs)     #returns term frequency\n",
        "   df_dict = wordDocFre(vocab, docs)             #returns document frequencies\n",
        "   idf_dict = inverseDocFre(vocab,df_dict,M)     #returns idf scores\n",
        "   tf_idf = tfidf(vocab,tf_dict,idf_dict,docs)   #returns tf-idf socres\n",
        "   \n",
        "   query1 = 'Text Mining'\n",
        "   query2 = 'LDA'\n",
        "   query3 = 'topic modeling'\n",
        "   query4 = 'Natural language Processing'\n",
        "   query5 = 'generative models'\n",
        "   top1 = vectorSpaceModel(query1, docs,tf_idf)    #returns top 5 documents using VSM\n",
        "   top2 = vectorSpaceModel(query2, docs,tf_idf)    #returns top 5 documents using VSM\n",
        "   top3 = vectorSpaceModel(query3, docs,tf_idf)    #returns top 5 documents using VSM\n",
        "   top4 = vectorSpaceModel(query4, docs,tf_idf)    #returns top 5 documents using VSM\n",
        "   top5 = vectorSpaceModel(query5, docs,tf_idf)    #returns top 5 documents using VSM\n",
        "   print('Top 5 Documents for Query 1: \\n', top1)\n",
        "   print('\\n')\n",
        "   print('Top 5 Documents for Query 2: \\n', top2)\n",
        "   print('\\n')\n",
        "   print('Top 5 Documents for Query 3: \\n', top3)\n",
        "   print('\\n')\n",
        "   print('Top 5 Documents for Query 4: \\n', top4)\n",
        "   print('\\n')\n",
        "   print('Top 5 Documents for Query 5: \\n', top5)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}